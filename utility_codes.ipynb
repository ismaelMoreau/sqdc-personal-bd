{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check if the special characters are in the filename\n",
    "            if '├й' in filename:\n",
    "                # Replace '├й' with 'é'\n",
    "                new_filename = filename.replace('├й', 'é')\n",
    "                # Form the full old and new file paths\n",
    "                old_filepath = os.path.join(root, filename)\n",
    "                new_filepath = os.path.join(root, new_filename)\n",
    "                # Rename the file\n",
    "                os.rename(old_filepath, new_filepath)\n",
    "                print(f'Renamed \"{old_filepath}\" to \"{new_filepath}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "rename_files('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check if the special characters are in the filename\n",
    "            if '├а' in filename:\n",
    "                # Replace '├й' with 'é'\n",
    "                new_filename = filename.replace('├а', 'à')\n",
    "                # Form the full old and new file paths\n",
    "                old_filepath = os.path.join(root, filename)\n",
    "                new_filepath = os.path.join(root, new_filename)\n",
    "                # Rename the file\n",
    "                os.rename(old_filepath, new_filepath)\n",
    "                print(f'Renamed \"{old_filepath}\" to \"{new_filepath}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "rename_files('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check if the special characters are in the filename\n",
    "            if '├к' in filename:\n",
    "                # Replace '├й' with 'é'\n",
    "                new_filename = filename.replace('├к', 'ê')\n",
    "                # Form the full old and new file paths\n",
    "                old_filepath = os.path.join(root, filename)\n",
    "                new_filepath = os.path.join(root, new_filename)\n",
    "                # Rename the file\n",
    "                os.rename(old_filepath, new_filepath)\n",
    "                print(f'Renamed \"{old_filepath}\" to \"{new_filepath}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "rename_files('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check if the special characters are in the filename\n",
    "            if '├Й' in filename:\n",
    "                # Replace '├й' with 'é'\n",
    "                new_filename = filename.replace('├Й', 'É')\n",
    "                # Form the full old and new file paths\n",
    "                old_filepath = os.path.join(root, filename)\n",
    "                new_filepath = os.path.join(root, new_filename)\n",
    "                # Rename the file\n",
    "                os.rename(old_filepath, new_filepath)\n",
    "                print(f'Renamed \"{old_filepath}\" to \"{new_filepath}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "rename_files('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Check if the special characters are in the filename\n",
    "            if '├и' in filename:\n",
    "                # Replace '├й' with 'é'\n",
    "                new_filename = filename.replace('├и', 'è')\n",
    "                # Form the full old and new file paths\n",
    "                old_filepath = os.path.join(root, filename)\n",
    "                new_filepath = os.path.join(root, new_filename)\n",
    "                # Rename the file\n",
    "                os.rename(old_filepath, new_filepath)\n",
    "                print(f'Renamed \"{old_filepath}\" to \"{new_filepath}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "rename_files('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_and_replace_pdf_with_png(directory):\n",
    "    # Walk through all directories and files in the given directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.pdf'):\n",
    "                # Construct full PDF file path\n",
    "                pdf_path = os.path.join(root, filename)\n",
    "                # Convert PDF to a list of images (one image per page)\n",
    "                images = convert_from_path(pdf_path)\n",
    "                # Assume we only want the first page for simplicity\n",
    "                if images:\n",
    "                    image_path = os.path.join(root, filename[:-4] + '.png')\n",
    "                    images[0].save(image_path, 'PNG')\n",
    "                    print(f'Converted \"{pdf_path}\" to \"{image_path}\"')\n",
    "                    # Delete the original PDF file\n",
    "                    os.remove(pdf_path)\n",
    "                    print(f'Deleted original PDF file \"{pdf_path}\"')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory you want to process\n",
    "convert_and_replace_pdf_with_png('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB validation schema\n",
    "hashish_schema = {\n",
    "    \"$jsonSchema\": {\n",
    "        \"bsonType\": \"object\",\n",
    "        \"required\": [\"Nom du Produit\",\"Marque\"],\n",
    "        \"properties\": {\n",
    "            \"Marque\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Marque du produit de hashish\"\n",
    "            },\n",
    "            \"Nom du Produit\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Nom commercial du produit de hashish\"\n",
    "            },\n",
    "            \"Format/Poids\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Poids du produit, par exemple '2g', '3.5g'\"\n",
    "            },\n",
    "            \"Variété/Souche\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Type de variété, par exemple 'Sativa', 'Hybride', 'Indica'\"\n",
    "            },\n",
    "            \"Génétique\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Description génétique du produit\"\n",
    "            },\n",
    "            \"Couleur Interne\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Couleur interne du hashish\"\n",
    "            },\n",
    "            \"Couleur Externe\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Couleur externe du hashish\"\n",
    "            },\n",
    "            \"Texture\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Texture du produit, par exemple 'malléable et résineuse'\"\n",
    "            },\n",
    "            \"Lieu de Culture\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Lieu où le cannabis a été cultivé\"\n",
    "            },\n",
    "            \"Méthode de Fabrication\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Méthode utilisée pour fabriquer le hashish, par exemple 'tamisage à sec', 'extraction au CO2'\"\n",
    "            },\n",
    "            \"Vieillissement\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Durée et conditions de vieillissement du produit\"\n",
    "            },\n",
    "            \"Effets Potentiels\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Effets attendus après consommation\"\n",
    "            },\n",
    "            \"Temps d'Apparition des Effets\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Délai d'apparition des effets après consommation\"\n",
    "            },\n",
    "            \"Message d'Avertissement\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Avertissements spécifiques à la consommation du produit\"\n",
    "            },\n",
    "            \"Cultivé\": {\n",
    "                \"bsonType\": \"bool\",\n",
    "                \"description\": \"Indique si le produit a été cultivé (Oui/Non)\"\n",
    "            },\n",
    "            \"Sans Solvant\": {\n",
    "                \"bsonType\": \"bool\",\n",
    "                \"description\": \"Indique si le produit a été fabriqué sans solvant (Oui/Non)\"\n",
    "            },\n",
    "            \"Autres Informations\": {\n",
    "                \"bsonType\": \"string\",\n",
    "                \"description\": \"Toute autre information pertinente non couverte par les autres champs\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to process each image\n",
    "def process_image(api_key, image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"de l'image, extrait les informations suivantes sous forme tableau avec le nom des carateristique en gras : Marque du produit, Nom du Produit, Format/Poids, Variété/Souche, Génétique, Couleur Interne, Couleur Externe, Texture, Lieu de Culture, Méthode de Fabrication, Vieillissement, Effets Potentiels, Temps d'Apparition des Effets, Message d'Avertissement, Cultivé, Sans Solvant, Autres Informations pertinentes\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Function to insert data into MongoDB\n",
    "def insert_into_mongo(collection, data):\n",
    "    try:\n",
    "        # Insert the dictionary into MongoDB\n",
    "        collection.insert_one(data)\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while inserting data:\", e)\n",
    "\n",
    "def extraire_tableau_du_texte(texte_complet):\n",
    "    # Trouver le début et la fin du tableau\n",
    "    debut_tableau = texte_complet.find(\"| **Marque du produit**\")  # Début du tableau\n",
    "    fin_tableau = texte_complet.find(\"| **Autres Informations**\")  # Fin du tableau\n",
    "\n",
    "    # Vérifier que les indices sont trouvés\n",
    "    if debut_tableau != -1 and fin_tableau != -1:\n",
    "        # Ajouter la longueur jusqu'à la fin de la ligne pour inclure tout le contenu de la dernière ligne\n",
    "        fin_de_la_ligne = texte_complet.find(\"\\n\", fin_tableau)\n",
    "        if fin_de_la_ligne != -1:\n",
    "            fin_tableau = fin_de_la_ligne + 1  # +1 pour inclure le saut de ligne\n",
    "\n",
    "        # Extraire le texte du tableau\n",
    "        texte_tableau = texte_complet[debut_tableau:fin_tableau].strip()\n",
    "        return texte_tableau\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extraire_informations_tableau(texte_tableau):\n",
    "    # Découper le texte en lignes\n",
    "    lignes = texte_tableau.split('\\n')\n",
    "\n",
    "    # Créer un dictionnaire pour stocker les informations\n",
    "    informations = {}\n",
    "\n",
    "    # Itérer sur les lignes pour extraire les informations, en ignorant la première ligne\n",
    "    for ligne in lignes[0:]:  # Commence à la deuxième ligne pour sauter les en-têtes\n",
    "        # Séparer la caractéristique et les détails\n",
    "        if '|' in ligne:\n",
    "            parts = ligne.split('|')\n",
    "            if len(parts) > 2:\n",
    "                # Nettoyer les caractéristiques et les détails pour éliminer les caractères spéciaux et les espaces superflus\n",
    "                caractéristique = parts[1].strip()#.replace('*', '').replace(':', '')\n",
    "                details = parts[2].strip()#.replace('*', '').replace(':', '')\n",
    "                informations[caractéristique] = details\n",
    "\n",
    "    return informations\n",
    "# Function to process all png files in a directory\n",
    "def process_directory(api_key, directory_path):\n",
    "    # Setup MongoDB connection\n",
    "    client = MongoClient(\"mongodb://127.0.0.1:27017/\")  # Adjust connection string as needed\n",
    "\n",
    "    db = client[\"sqdc_quaie\"]  \n",
    "    \n",
    "    collection_name = \"hashish\"\n",
    "    if collection_name in db.list_collection_names():\n",
    "        print(f\"La collection '{collection_name}' existe déjà.\")\n",
    "        collection = db[collection_name]\n",
    "    else:\n",
    "        collection = db.create_collection(\"hashish\", validator=hashish_schema,validationAction='warn')\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "            result = process_image(api_key, image_path)\n",
    "            print(result)\n",
    "            # Assume that the extracted data is in 'result' and insert it\n",
    "            tableau = extraire_tableau_du_texte(result['choices'][0]['message']['content'])\n",
    "            print(tableau)\n",
    "            data = extraire_informations_tableau(tableau)\n",
    "            \n",
    "            \n",
    "            insert_into_mongo(collection, data)\n",
    "            print(f\"Results for {filename}: {data}\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get('sqdc_api_key')\n",
    "directory_path = \"./pdf_products/Hashish\"\n",
    "process_directory(api_key, directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def export_to_excel(db_name, collection_name, output_file):\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client['sqdc_quaie']\n",
    "    collection = db['hashish']\n",
    "    \n",
    "    # Query all documents in the collection\n",
    "    data = list(collection.find())\n",
    "    print(data)\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    # Optionally, you can drop the MongoDB ID field if not needed\n",
    "    if '_id' in df.columns:\n",
    "        df.drop('_id', axis=1, inplace=True)\n",
    "    \n",
    "    # Write to Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Data exported successfully to {output_file}\")\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "export_to_excel(\"sqdc_quai\", \"hashish\", \"output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ.get('sqdc_api_key')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# headers =   {\n",
    "#     'accept-language': 'en-CA', #import to keep this header for some reason\n",
    "#     'x-requested-with':'XMLHttpRequest',\n",
    "#     \"user-agent\": \"SQDC-Back-in-Stock-Notification\"#import to keep this header\n",
    "#     }\n",
    "\n",
    "# url = 'https://www.sqdc.ca/api/inventory/findInventoryItems'\n",
    "# payload = {\"Sku\":\"671148904118\",\"Page\":1,\"Pagesize\":1000} #Pagesize is basically number of stores, get all stores with 1000, SKU comes from url\n",
    "\n",
    "# resp = requests.post(url,headers=headers,json=payload).json()\n",
    "# print(len(resp['Stores']))\n",
    "\n",
    "# inventory = {x['Name']:x['InventoryStatus']['Quantity'] for x in resp['Stores']} #store_name : inventory count\n",
    "# print(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to process each image\n",
    "def process_image(api_key, image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"De l'image, extrait les informations sous forme de JSON pour insérer dans MongoDB, aucun niveau d'imbrication, en français et seulement des chaînes de caractères ou des tableaux pour les listes. Les champs obligatoires sont 'nom' et 'marque'.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Function to insert data into MongoDB\n",
    "def insert_into_mongo(collection, data):\n",
    "    try:\n",
    "        # Insert the dictionary into MongoDB\n",
    "        collection.insert_one(data)\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while inserting data:\", e)\n",
    "\n",
    "def extraire_code(response):\n",
    "    product_block = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "    if not product_block:\n",
    "        raise ValueError(f\"No product detail block found in the response on : {response}\")\n",
    "\n",
    "    product_details = product_block.group(1).strip()\n",
    "    lines = product_details.split('\\n')\n",
    "    if lines:\n",
    "        # Remove the first line which is the identifier\n",
    "        cleaned_details = '\\n'.join(lines[1:]).strip()\n",
    "    else:\n",
    "        cleaned_details = product_details\n",
    "\n",
    "    # Parse product details into a dictionary\n",
    "    product_info = {}\n",
    "    for line in cleaned_details.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            product_info[key.strip().strip('\"')] = value.strip().strip('\",').replace('\\\\\"', '\"')\n",
    "\n",
    "    # Clean up lists that were detected as strings\n",
    "    for key, value in product_info.items():\n",
    "        if value.startswith('[') and value.endswith(']'):\n",
    "            product_info[key] = json.loads(value)\n",
    "\n",
    "    return product_info\n",
    "\n",
    "# Define the schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"nom\": {\"type\": \"string\"},\n",
    "        \"marque\": {\"type\": \"string\"},\n",
    "        \"type_de_produit\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"nom\", \"marque\", \"type_de_produit\"]\n",
    "}\n",
    "\n",
    "# Function to validate the data against the schema\n",
    "def validate_data(data):\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation error: {e.message}\")\n",
    "        return False\n",
    "\n",
    "# Function to process all png files in a directory\n",
    "def process_directory(api_key, directory_path):\n",
    "    # Setup MongoDB connection\n",
    "    client = MongoClient(\"mongodb://127.0.0.1:27017/\")  # Adjust connection string as needed\n",
    "    db = client[\"sqdc_quaie\"]\n",
    "\n",
    "    # Use the directory name as the collection name\n",
    "    collection_name = os.path.basename(directory_path)\n",
    "\n",
    "    # Define the schema for MongoDB\n",
    "    base_schema = {\n",
    "        \"$jsonSchema\": {\n",
    "            \"bsonType\": \"object\",\n",
    "            \"required\": [\"nom\", \"marque\", \"type_de_produit\"],\n",
    "            \"properties\": {\n",
    "                \"nom\": {\n",
    "                    \"bsonType\": \"string\",\n",
    "                    \"description\": \"Le nom du produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                },\n",
    "                \"marque\": {\n",
    "                    \"bsonType\": \"string\",\n",
    "                    \"description\": \"La marque du produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                },\n",
    "                \"type_de_produit\": {\n",
    "                    \"bsonType\": \"string\",\n",
    "                    \"description\": \"Le type de produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if collection_name in db.list_collection_names():\n",
    "        print(f\"La collection '{collection_name}' existe déjà.\")\n",
    "        collection = db[collection_name]\n",
    "    else:\n",
    "        collection = db.create_collection(collection_name, validator=base_schema, validationAction='warn')\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "            result = process_image(api_key, image_path)\n",
    "            code_block = extraire_code(result['choices'][0]['message']['content'])\n",
    "\n",
    "            # Add the directory name as the type_de_produit\n",
    "            code_block['type_de_produit'] = collection_name\n",
    "\n",
    "            # Validate the data before inserting into MongoDB\n",
    "            if validate_data(code_block):\n",
    "                insert_into_mongo(collection, code_block)\n",
    "            else:\n",
    "                print(f\"Data for {filename} does not conform to the schema and will not be inserted.\")\n",
    "\n",
    "api_key = os.environ.get('sqdc_api_key')\n",
    "directory_path = \"./pdf_products/Huiles\"\n",
    "results = process_directory(api_key, directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json{\n",
    "    \"NomProduit\": \"OLLOPA Big Bang Atomiseur\",\n",
    "    \"VolumeNet\": \"15ML\",\n",
    "    \"UtilisationEtEffets\": {\n",
    "        \"ImpressionDeRessentirDesSensationsStimulantes\": true,\n",
    "        \"SentimentDeJoie\": true,\n",
    "        \"StimulationDeFonctionsCerebrales\": true,\n",
    "        \"SentimentDEuphorie\": true\n",
    "    },\n",
    "    \"Ingredients\": [\n",
    "        \"Triglycérides à chaîne moyenne\",\n",
    "        \"Extrait d'huile de cannabis\"\n",
    "    ],\n",
    "    \"MethodeExtraction\": \"Éthanol cryogénique. Fait à partir d'un distillat de THC.\",\n",
    "    \"MiseEnGarde\": \"Les effets de la consommation de produits comestibles à base de cannabis peuvent être de longue durée. Les effets peuvent durer de six à douze heures après la consommation.\",\n",
    "    \"Labels\": [\n",
    "        \"Produit du Québec\",\n",
    "        \"Pur Cann Pharma\",\n",
    "        \"Un membre de groupe Silicycle\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory_path):\n",
    "    if filename.lower().endswith(\".png\") :\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to process each image\n",
    "def process_image(api_key, image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"De l'image, extrait les informations sous forme de JSON pour insérer dans MongoDB, aucun niveau d'imbrication, en français et seulement des chaînes de caractères ou des tableaux pour les listes. Les champs obligatoires sont 'nom' et 'marque'.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Function to insert data into MongoDB\n",
    "def insert_into_mongo(collection, data, filename):\n",
    "    try:\n",
    "        collection.insert_one(data)\n",
    "        print(f\"Data of {filename} inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting data of {filename}:\", e)\n",
    "\n",
    "def extraire_code(response):\n",
    "    product_block = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "    if not product_block:\n",
    "        print(\"this response wasn't inserted into mongo :\")\n",
    "        print(response)\n",
    "        return None\n",
    "\n",
    "    product_details = product_block.group(1).strip()\n",
    "    lines = product_details.split('\\n')\n",
    "    if lines:\n",
    "        # Remove the first line which is the identifier\n",
    "        cleaned_details = '\\n'.join(lines[1:]).strip()\n",
    "    else:\n",
    "        cleaned_details = product_details\n",
    "\n",
    "    # Parse product details into a dictionary\n",
    "    product_info = {}\n",
    "    for line in cleaned_details.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            product_info[key.strip().strip('\"')] = value.strip().strip('\",').replace('\\\\\"', '\"')\n",
    "\n",
    "    for key, value in product_info.items():\n",
    "        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "            try:\n",
    "                product_info[key] = json.loads(value)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle the case where value is not a valid JSON string\n",
    "                print(f\"Error decoding JSON for {key}: {e}\")\n",
    "\n",
    "    return product_info\n",
    "\n",
    "# Define the schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"nom\": {\"type\": \"string\"},\n",
    "        \"marque\": {\"type\": \"string\"},\n",
    "        \"type_de_produit\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"nom\", \"marque\", \"type_de_produit\"]\n",
    "}\n",
    "\n",
    "# Function to validate the data against the schema\n",
    "def validate_data(data):\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation error: {e.message}\")\n",
    "        return False\n",
    "\n",
    "# Function to process all png files in a directory\n",
    "def process_directory(api_key, directory_path):\n",
    "    # Setup MongoDB connection\n",
    "    client = MongoClient(\"mongodb://127.0.0.1:27017/\")  # Adjust connection string as needed\n",
    "    db = client[\"sqdc_quaie\"]\n",
    "\n",
    "    # Iterate over each subdirectory\n",
    "    for subdir_name in os.listdir(directory_path):\n",
    "        subdir_path = os.path.join(directory_path, subdir_name)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            collection_name = os.path.basename(subdir_path)\n",
    "            # Define the schema for MongoDB\n",
    "            base_schema = {\n",
    "                \"$jsonSchema\": {\n",
    "                    \"bsonType\": \"object\",\n",
    "                    \"required\": [\"nom\", \"marque\", \"type_de_produit\"],\n",
    "                    \"properties\": {\n",
    "                        \"nom\": {\n",
    "                            \"bsonType\": \"string\",\n",
    "                            \"description\": \"Le nom du produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                        },\n",
    "                        \"marque\": {\n",
    "                            \"bsonType\": \"string\",\n",
    "                            \"description\": \"La marque du produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                        },\n",
    "                        \"type_de_produit\": {\n",
    "                            \"bsonType\": \"string\",\n",
    "                            \"description\": \"Le type de produit est obligatoire et doit être une chaîne de caractères\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if collection_name in db.list_collection_names():\n",
    "                print(f\"La collection '{collection_name}' existe déjà.\")\n",
    "                count = db[collection_name].count_documents({})\n",
    "                file_count = len([name for name in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, name))])\n",
    "                if count != file_count:\n",
    "                    print(f\"Le nombre d'entree de la collection '{collection_name}' ne correspond pas au nombre de png.\")\n",
    "                    db[collection_name].drop()\n",
    "                    collection = db.create_collection(collection_name, validator=base_schema, validationAction='warn')\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                collection = db.create_collection(collection_name, validator=base_schema, validationAction='warn')\n",
    "\n",
    "            for filename in os.listdir(subdir_path):\n",
    "                if filename.lower().endswith(\".png\"):\n",
    "                    image_path = os.path.join(subdir_path, filename)\n",
    "                    result = process_image(api_key, image_path)\n",
    "                    if 'choices' in result:\n",
    "                        code_block = extraire_code(result['choices'][0]['message']['content'])\n",
    "                    else:\n",
    "                        print(f\"No Data extracted for {filename}\")\n",
    "                    if code_block:\n",
    "                        # Add the directory name as the type_de_produit\n",
    "                        code_block['type_de_produit'] = collection_name\n",
    "\n",
    "                        # Validate the data before inserting into MongoDB\n",
    "                        if validate_data(code_block):\n",
    "                            insert_into_mongo(collection, code_block, filename)\n",
    "                        else:\n",
    "                            print(f\"Data for {filename} does not conform to the schema and will not be inserted.\")\n",
    "                    \n",
    "\n",
    "api_key = os.environ.get('sqdc_api_key')\n",
    "directory_path = \"./pdf_products\"\n",
    "process_directory(api_key, directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "def listes_des_champs_regrouper(api_key, champs_a_regrouper):\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"de ma liste de champs, fais le schema de validation MongoDB complet pour que les prochaines entrées soient uniformisées en utilisant seulement des strings comme type, aucun champs dupliqués et en francais: {champs_a_regrouper}\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def get_all_fields(collection):\n",
    "    # Initialize a set to keep track of all distinct keys\n",
    "    keys = set()\n",
    "\n",
    "    # Iterate over each document in the collection\n",
    "    for document in collection.find():\n",
    "        # Update the set with the keys of the current document\n",
    "        keys.update(document.keys())\n",
    "\n",
    "    # Convert the set to a list (optional, as sets already contain unique elements)\n",
    "    return list(keys)\n",
    "\n",
    "\n",
    "# def update_and_remove_fields(grouped_fields, collection):\n",
    "#     for group_key, fields in grouped_fields.items():\n",
    "#         if group_key == '_id':\n",
    "#             continue  # Skip the immutable _id field\n",
    "        \n",
    "#         for field in fields:\n",
    "#             # Update each document by setting the group_key to the value of the field and then removing the field\n",
    "#             collection.update_many(\n",
    "#                 {field: {\"$exists\": True}},  # Only update if the field exists\n",
    "#                 [\n",
    "#                     {\n",
    "#                         '$set': {\n",
    "#                             group_key: {\n",
    "#                                 '$concatArrays': [\n",
    "#                                     {\"$cond\": {\"if\": {\"$isArray\": f\"${group_key}\"}, \"then\": f\"${group_key}\", \"else\": []}},\n",
    "#                                     {\"$cond\": {\"if\": {\"$isArray\": f\"${field}\"}, \"then\": f\"${field}\", \"else\": [\"$\" + field]}}\n",
    "#                                 ]\n",
    "#                             }\n",
    "#                         }\n",
    "#                     },\n",
    "#                     {'$unset': field}  # Remove the original field\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "\n",
    "def extraire_code(response):\n",
    "    product_block = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "    if not product_block:\n",
    "        print(\"this response wasn't inserted into mongo :\")\n",
    "        print(response)\n",
    "        return None\n",
    "\n",
    "    product_details = product_block.group(1).strip()\n",
    "    lines = product_details.split('\\n')\n",
    "    if lines:\n",
    "        # Remove the first line which is the identifier\n",
    "        cleaned_details = '\\n'.join(lines[1:]).strip()\n",
    "    else:\n",
    "        cleaned_details = product_details\n",
    "\n",
    "    #Parse product details into a dictionary\n",
    "    product_info = {}\n",
    "    for line in cleaned_details.split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            product_info[key.strip().strip('\"')] = value.strip().strip('\",').replace('\\\\\"', '\"')\n",
    "\n",
    "    for key, value in product_info.items():\n",
    "        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "            try:\n",
    "                product_info[key] = json.loads(value)\n",
    "            except json.JSONDecodeError as e:\n",
    "                # Handle the case where value is not a valid JSON string\n",
    "                print(f\"Error decoding JSON for {key}: {e}\")\n",
    "\n",
    "    return product_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connexion à MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sqdc_quaie']\n",
    "collection = db['Autres Comestibles']\n",
    "api_key = os.environ.get('sqdc_api_key')\n",
    "\n",
    "\n",
    "fields = get_all_fields(collection)\n",
    "response = listes_des_champs_regrouper(api_key,fields)\n",
    "if 'choices' in response:\n",
    "    grouped_fields = extraire_code(response['choices'][0]['message']['content'])\n",
    "    #update_group_fields_in_mongo(grouped_fields,collection)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pour créer un schéma de validation dans MongoDB qui impose que toutes les entrées pour chaque champ mentionné ci-dessus soient des chaînes (*strings*), sans champs dupliqués, en utilisant un texte francais, vous pouvez utiliser le validateur de schéma MongoDB comme démontré ci-dessous. \\n\\nNotez que \"ingrédients\" et \"ingredients\" semblent être des doublons. Vous devriez normaliser votre base de données en utilisant un seul de ces noms pour éviter des erreurs et confusions. Je vais utiliser \"ingrédients\" car elle respecte la commande d\\'utiliser l\\'orthographe en francais. \\n\\nVoici un exemple de validation de schéma : \\n\\n```js\\ndb.createCollection(\"maCollection\", {\\n   validator: {\\n      $jsonSchema: {\\n         bsonType: \"object\",\\n         required: [\"effets potentiels\", \"package\", \"poids\", \"_id\", \"quantite\", \"mode de consommation\", \"taux de THC\", \"disponibilite\", \"mode\", \"presentations\", \"sans\", \"temps de réaction\", \"message d\\'avertissement\", \"marque\", \"type\", \"produit_id\", \"description\", \"format\", \"ingrédients\", \"type_de_produit\", \"capacité\", \"allergenes\", \"efficacite\", \"nom\", \"attributs\"],\\n         properties: {\\n            \"effets potentiels\": {\\n               bsonType: \"string\"\\n            },\\n            \"package\": {\\n               bsonType: \"string\"\\n            },\\n            \"poids\": {\\n               bsonType: \"string\"\\n            },\\n            \"_id\": {\\n               bsonType: \"string\"\\n            },\\n            \"quantite\": {\\n               bsonType: \"string\"\\n            },\\n            \"mode de consommation\": {\\n               bsonType: \"string\"\\n            },\\n            \"taux de THC\": {\\n               bsonArte: \"string\"\\n            },\\n            \"disponibilite\": {\\n               bsonType: \"string\"\\n            },\\n            \"mode\": {\\n               bsonType: \"string\"\\n            },\\n            \"presentations\": {\\n               bsonType: \"string\"\\n            },\\n            \"sans\": {\\n               bsonYype: \"string\"\\n            },\\n            \"temps de réaction\": {\\n               bsonType: \"string\"\\n            },\\n            \"message d\\'avertissement\": {\\n               bsonType: \"string\"\\n            },\\n            \"marque\": {\\n               bsonType: \"string\"\\n            },\\n            \"type\": {\\n               bsonType: \"string\"\\n            },\\n            \"produit_id\": {\\n               bsonType: \"string\"\\n            },\\n            \"description\": {\\n               bsonType: \"string\"\\n            },\\n            \"format\": {\\n               bsonType: \"string\"\\n            },\\n            \"ingrédients\": {\\n               bsonType: \"string\"\\n            },\\n            \"type_de_produit\": {\\n               bsonType: \"string\"\\n            },\\n            \"capacité\": {\\n               bsonType: \"string\"\\n            },\\n            \"allergenes\": {\\n                bsonType: \"string\"\\n            },\\n            \"efficacite\": {\\n               bsonType: \"finding\"\\n            },\\n            \"nom\": {\\n               bsonType: \"string\"\\n            },\\n            \"attributs\": {\\n               bsonType: \"string\"\\n            }\\n         }\\n      }\\n   }\\n});\\n```\\n\\nDans cet exemple, chaque champ est requis (`required`) et chaque type de données pour ces champs est défini comme étant une chaîne (`bsonType: \"string\"`). Assurez-vous que dans l\\'application client, toutes les données envoyées à MongoDB sont bien des chaînes pour éviter des erreurs de validation. \\n\\nLa commande `createCollection` crée une nouvelle collection avec le schéma de validation spécifié. Si la collection existe déjà, vous devez utiliser la commande `collMod` pour ajouter ou ajuster la validation de schéma.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #grouped_fields = extraire_code(response['choices'][0]['message']['content'])\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validator': '{',\n",
       " '$jsonSchema': '{',\n",
       " 'bsonType': 'string',\n",
       " 'required': ['effets potentiels',\n",
       "  'package',\n",
       "  'poids',\n",
       "  '_id',\n",
       "  'quantite',\n",
       "  'mode de consommation',\n",
       "  'taux de THC',\n",
       "  'disponibilite',\n",
       "  'mode',\n",
       "  'presentations',\n",
       "  'sans',\n",
       "  'temps de réaction',\n",
       "  \"message d'avertissement\",\n",
       "  'marque',\n",
       "  'type',\n",
       "  'produit_id',\n",
       "  'description',\n",
       "  'format',\n",
       "  'ingrédients',\n",
       "  'type_de_produit',\n",
       "  'capacité',\n",
       "  'allergenes',\n",
       "  'efficacite',\n",
       "  'nom',\n",
       "  'attributs'],\n",
       " 'properties': '{',\n",
       " 'effets potentiels': '{',\n",
       " 'package': '{',\n",
       " 'poids': '{',\n",
       " '_id': '{',\n",
       " 'quantite': '{',\n",
       " 'mode de consommation': '{',\n",
       " 'taux de THC': '{',\n",
       " 'bsonArte': 'string',\n",
       " 'disponibilite': '{',\n",
       " 'mode': '{',\n",
       " 'presentations': '{',\n",
       " 'sans': '{',\n",
       " 'bsonYype': 'string',\n",
       " 'temps de réaction': '{',\n",
       " \"message d'avertissement\": '{',\n",
       " 'marque': '{',\n",
       " 'type': '{',\n",
       " 'produit_id': '{',\n",
       " 'description': '{',\n",
       " 'format': '{',\n",
       " 'ingrédients': '{',\n",
       " 'type_de_produit': '{',\n",
       " 'capacité': '{',\n",
       " 'allergenes': '{',\n",
       " 'efficacite': '{',\n",
       " 'nom': '{',\n",
       " 'attributs': '{'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sqdc_quaie']\n",
    "collection = db['your_collection_name']\n",
    "\n",
    "# Define the document\n",
    "document = {\n",
    "    \"nom\": \"PINK KUSH\",\n",
    "    \"marque\": \"GCGold Tech\",\n",
    "    \"variété\": \"OG Kush x Inconnue\",\n",
    "    \"type\": \"Indica\",\n",
    "    \"quantité\": \"3,5 gr\",\n",
    "    \"caractéristiques\": [\n",
    "        \"Cultivé 100% Québec\",\n",
    "        \"Culture et Sécheur sur l'Île Brossard\",\n",
    "        \"Récolte contrôlée automatiquement dans une chambre de fermentation\",\n",
    "        \"48 heures avant l'emballage\"\n",
    "    ],\n",
    "    \"effets_potentiels\": [\n",
    "        \"Relaxant\",\n",
    "        \"Euphorie\",\n",
    "        \"Sensations accrues\"\n",
    "    ],\n",
    "    \"avertissement\": \"Ne conduisez pas un opérez pas de machinerie lourde après avoir consommé du cannabis\"\n",
    "}\n",
    "\n",
    "# Insert the document into the collection\n",
    "result = collection.insert_one(document)\n",
    "\n",
    "# Print the ID of the inserted document\n",
    "print(\"Document inserted with ID:\", result.inserted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def remove_conditional_fields(document):\n",
    "    # Iterate over all fields in the document\n",
    "    for key in list(document.keys()):\n",
    "        if isinstance(document[key], dict):\n",
    "            # Check if the field contains a $cond\n",
    "            if '$cond' in document[key]:\n",
    "                del document[key]  # Remove the field with $cond\n",
    "            else:\n",
    "                # Recursively check nested dictionaries\n",
    "                remove_conditional_fields(document[key])\n",
    "\n",
    "def update_documents(collection):\n",
    "    # Fetch all documents from the collection\n",
    "    documents = collection.find()\n",
    "\n",
    "    for document in documents:\n",
    "        # Remove conditional fields from each document\n",
    "        remove_conditional_fields(document)\n",
    "        \n",
    "        # Update the document in the collection\n",
    "        collection.replace_one({'_id': document['_id']}, document)\n",
    "\n",
    "# Example usage:\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['sqdc_quaie']\n",
    "collection = db['Atomiseurs']\n",
    "\n",
    "update_documents(collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def remove_placeholder_fields(document):\n",
    "    # Create a copy of the document to avoid modifying the original dictionary during iteration\n",
    "    document_copy = document.copy()\n",
    "    \n",
    "    # Iterate over all fields in the document\n",
    "    for key, value in document_copy.items():\n",
    "        if isinstance(value, str) and value.startswith('$'):\n",
    "            del document[key]  # Remove the field with placeholder value\n",
    "        elif isinstance(value, dict):\n",
    "            # Recursively check nested dictionaries\n",
    "            remove_placeholder_fields(value)\n",
    "        elif isinstance(value, list):\n",
    "            # Check each item in the list for nested dictionaries\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    remove_placeholder_fields(item)\n",
    "\n",
    "def update_documents(collection):\n",
    "    # Fetch all documents from the collection\n",
    "    documents = collection.find()\n",
    "\n",
    "    for document in documents:\n",
    "        # Remove placeholder fields from each document\n",
    "        remove_placeholder_fields(document)\n",
    "        \n",
    "        # Update the document in the collection\n",
    "        collection.replace_one({'_id': document['_id']}, document)\n",
    "\n",
    "# Example usage:\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['sqdc_quaie']\n",
    "collection = db['Boissons']\n",
    "\n",
    "update_documents(collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def export_collection_to_excel(collection, excel_file_path):\n",
    "    # Fetch all documents from the collection\n",
    "    documents = list(collection.find())\n",
    "\n",
    "    if documents:\n",
    "        # Convert the documents to a DataFrame\n",
    "        df = pd.DataFrame(documents)\n",
    "\n",
    "        # Export the DataFrame to an Excel file\n",
    "        df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "        print(f\"Data exported successfully to {excel_file_path}\")\n",
    "    else:\n",
    "        print(\"No documents found in the collection.\")\n",
    "\n",
    "# Example usage:\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['sqdc_quaie']\n",
    "collection = db['Bouchées']\n",
    "export_collection_to_excel(collection, 'Bouchées.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n",
      "No valid response from API\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqdc_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m db_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqdc_quaie\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mextract_and_save_schemas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[89], line 67\u001b[0m, in \u001b[0;36mextract_and_save_schemas\u001b[0;34m(api_key, db_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m fields \u001b[38;5;241m=\u001b[39m get_all_fields(collection)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Request schema extraction from OpenAI API\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlistes_des_champs_regrouper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Extract schema from API response\u001b[39;00m\n\u001b[1;32m     70\u001b[0m schema \u001b[38;5;241m=\u001b[39m extraire_code(response)\n",
      "Cell \u001b[0;32mIn[89], line 25\u001b[0m, in \u001b[0;36mlistes_des_champs_regrouper\u001b[0;34m(api_key, champs_a_regrouper)\u001b[0m\n\u001b[1;32m      8\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     ]\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.openai.com/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    506\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def listes_des_champs_regrouper(api_key, champs_a_regrouper):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are an mongodb schema creator, you will only response in a mongodb schema validator\"\n",
    "            },\n",
    "            {\n",
    "                \"role\" :  \"user\",\n",
    "                \"content\" : f\"Voici une collection utilisant les champs suivants : {champs_a_regrouper}. Assurez-vous que les prochaines entrées sont uniformisées en utilisant uniquement des chaînes de caractères comme types de données, sans champs dupliqués, en français et en format json.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def extraire_code(response):\n",
    "    if 'choices' in response and response['choices']:\n",
    "        message_content = response['choices'][0]['message']['content']\n",
    "        product_block = re.search(r'```json\\n(.*?)\\n```', message_content, re.DOTALL)\n",
    "        if product_block:\n",
    "            product_details = product_block.group(1).strip()\n",
    "            try:\n",
    "                # Attempt to load the JSON, ensuring proper handling of double quotes\n",
    "                schema = json.loads(product_details.replace(\"'\", '\"'))\n",
    "                return schema\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                print(f\"JSON content: {product_details}\")\n",
    "        else:\n",
    "            print(\"No valid JSON block found in API response\")\n",
    "            print(f\"Response content: {message_content}\")\n",
    "    else:\n",
    "        print(\"No valid response from API\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_schema_to_json(schema, collection_name):\n",
    "    filename = f\"{collection_name}_schema.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(schema, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Schema for collection '{collection_name}' saved to '{filename}'\")\n",
    "\n",
    "def extract_and_save_schemas(api_key, db_name):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    # Get all collection names in the database\n",
    "    collection_names = db.list_collection_names()\n",
    "\n",
    "    for collection_name in collection_names:\n",
    "        collection = db[collection_name]\n",
    "        fields = get_all_fields(collection)\n",
    "        \n",
    "        # Request schema extraction from OpenAI API\n",
    "        response = listes_des_champs_regrouper(api_key, fields)\n",
    "        \n",
    "        # Extract schema from API response\n",
    "        schema = extraire_code(response)\n",
    "        if schema:\n",
    "            # Save schema to JSON file\n",
    "            save_schema_to_json(schema, collection_name)\n",
    "\n",
    "def get_all_fields(collection):\n",
    "    # Initialize a set to keep track of all distinct keys\n",
    "    keys = set()\n",
    "\n",
    "    # Iterate over each document in the collection\n",
    "    for document in collection.find():\n",
    "        # Update the set with the keys of the current document\n",
    "        keys.update(document.keys())\n",
    "\n",
    "    # Convert the set to a list (optional, as sets already contain unique elements)\n",
    "    return list(keys)\n",
    "\n",
    "# Example usage:\n",
    "api_key = os.environ.get('sqdc_api_key')\n",
    "db_name = 'sqdc_quaie'\n",
    "extract_and_save_schemas(api_key, db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Résine Rosine: 28\n",
      "len Boissons: 89\n",
      "len Fleurs Séchées  1g: 34\n",
      "len Fleurs Séchées Rotation 3.5g: 297\n",
      "len Pré-roulés Indica: 291\n",
      "len Atomiseurs: 40\n",
      "len Hashish: 230\n",
      "len Huiles: 95\n",
      "len Pré-roulés Hybride: 431\n",
      "len Fleurs Séchées Hybride 3.5g: 604\n",
      "len Thé - Tisanes: 50\n",
      "len Fleurs Séchées  Sativa 3.5g: 391\n",
      "len Pré-roulés en Rotation-Mélange: 219\n",
      "len Fleurs Séchées Indica 3.5g: 392\n",
      "len Pré-roulés Sativa: 338\n",
      "len Bouchées: 120\n",
      "len Kief: 41\n",
      "len Fleurs Séchées 15-28g: 286\n",
      "len Pré-roulés Concentrés: 171\n",
      "len Moulu: 112\n",
      "len your_collection_name: 9\n",
      "len Capsules: 68\n",
      "len Autres Comestibles: 26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def get_all_fields(collection):\n",
    "    # Initialize a set to keep track of all distinct keys\n",
    "    keys = set()\n",
    "\n",
    "    # Iterate over each document in the collection\n",
    "    for document in collection.find():\n",
    "        # Update the set with the keys of the current document\n",
    "        keys.update(document.keys())\n",
    "\n",
    "    # Convert the set to a list (optional, as sets already contain unique elements)\n",
    "    return list(keys)\n",
    "def save_schema_to_json(schema, collection_name):\n",
    "    filename = f\"{collection_name}_fields.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(schema, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Schema for collection '{collection_name}' saved to '{filename}'\")\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sqdc_quaie']\n",
    "    \n",
    "    # Get all collection names in the database\n",
    "collection_names = db.list_collection_names()\n",
    "\n",
    "for collection_name in collection_names:\n",
    "    collection = db[collection_name]\n",
    "    fields = get_all_fields(collection)\n",
    "    print(f\"len {collection_name}: {len(fields)}\")\n",
    "    \n",
    "    #save_schema_to_json(fields, collection_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 40 columns with the fewest NaN values:\n",
      "Index(['_id', 'nom', 'marque', 'type_de_produit', 'type', 'effets',\n",
      "       'effets_potentiels', 'format', 'origine', 'variété', 'culture',\n",
      "       'description', 'poids', 'lieu_de_culture', 'génétique', 'quantité',\n",
      "       'avertissement', 'méthode_de_culture', 'THC', 'producteur', 'genetique',\n",
      "       'temps_d'apparition_des_effets', 'emballage', 'CBD', 'variete',\n",
      "       'temps_apparition_effets', 'avertissements', 'dominance', 'couleur',\n",
      "       'categorie', 'humidité', 'utilisation', 'souche', 'produit', 'cultivé',\n",
      "       'forme', 'odeur', 'arômes', 'espèce', 'conservation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "# MongoDB connection details\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "database_name = \"sqdc_quaie\"\n",
    "collection_name = \"Fleurs Séchées Hybride 3.5g\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(mongo_uri)\n",
    "db = client[database_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Retrieve all documents from the collection\n",
    "documents = collection.find()\n",
    "\n",
    "# Convert the documents to a list of dictionaries\n",
    "documents_list = list(documents)\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(documents_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "nan_counts = df.isna().sum()\n",
    "\n",
    "# Sort the counts in ascending order and select the top 10 columns with the fewest NaN values\n",
    "top_10_columns_fewest_nan = nan_counts.nsmallest(40).index\n",
    "\n",
    "# Display the top 10 columns with the fewest NaN values\n",
    "print(\"\\nTop 40 columns with the fewest NaN values:\")\n",
    "print(top_10_columns_fewest_nan)\n",
    "\n",
    "\n",
    "\n",
    "# champs_unifies = {\n",
    "#     'nom': ['nom_produit', 'nom'],\n",
    "#     'marque': ['marque', 'logo'],\n",
    "#     'type_de_produit': ['type_produit', 'type_de_produit', 'typeProduit'],\n",
    "#     'effets': ['effets', 'effets_potentiels', 'effets_mentaux', 'effets_estimes', 'effets_reportés', 'effets_apres_inhaltion', 'effets_humains', 'Effets_attendus', 'Effets après l\\'inhalation', 'efets_potentiels', 'effets potentiels', 'Effets potentiels', 'effet', 'Effets après l\\'inhaltion'],\n",
    "#     'format': ['format', 'Format'],\n",
    "#     'origine': ['origine', 'province_d_origine', 'lieu_de_production', 'lieu_de_culture', 'lieu_de_culture', 'provenance'],\n",
    "#     'variété': ['variete', 'Variété', 'varietes', 'variété', 'variété_de_cannabis', 'variete_du_cannabis'],\n",
    "#     'culture': ['culture', 'culture_et_production', 'methodes_de_culture', 'methode_de_plantage', 'culture et production ', 'method_culture', 'Culture du climat', 'methode_changement_lumiere', 'methode_d_affinage', 'culture_production', 'Méthode_de_culture_et_production', 'methode', 'Méthode de culture', 'methode_culture', 'methodes_de_culture', 'culture_et_production', 'methode_de_culture', 'culture et production', 'culture_d\\'appartation', 'culture', 'Méthode de culture', 'conditions_de_culture', 'Méthode de culture', 'methode_de_culture', 'méthode_de_culture', 'Méthode de culture', 'methode_affinage', 'methode_d\\'affinage', 'method_culture', 'méthode_d\\'affinage'],\n",
    "#     'description': ['description', 'description_produit', 'description_fleurs', 'description_fleurs', 'description_produit'],\n",
    "#     'poids': ['poids', 'poids_net', 'poids_net', 'poids'],\n",
    "#     'génétique': ['génétique', 'Genétique', 'genetiques', 'gensetique', 'parents_génétiques'],\n",
    "#     'quantité': ['quantité', 'quantité_par_sachet', 'quantités_disponibles', 'quantite'],\n",
    "#     'avertissement': ['avertissement', 'mention_avertissement', 'messages_avertissements', 'advertissement', 'messages d\\'avertissement', 'message_avertissement', 'message_d_avertissement', 'messages_d\\'avertissement', 'avertissements', 'attention', 'warnings', 'consommation_avertie', 'mise_en_garde', 'disclaimer', 'stipulations', 'consommation', 'Messages_d\\'avertissements'],\n",
    "#     'THC': ['pourcentage THC', 'contenu_de_THC', 'taux_de_THC', 'contenuTHC', 'taux_thc', 'teneur_en_thc', 'intensité_du_THC', 'teneurTHC', 'taux_de_thc', 'puissance_THC', 'quantite_thc', 'deTauxTHC', 'puissance', 'tetrahydrocannabinol', 'niveau_thc', 'pourcentage_de_thc'],\n",
    "#     'producteur': ['producteur', 'Cultivé par', 'nom_du_cultivateur'],\n",
    "#     'emballage': ['emballage', 'packaging', 'sachet_hermétique', 'sachet_humidite_inclu'],\n",
    "#     'CBD': ['pourcentage_de_cbd', 'contenuCBD', 'contenu_CBD', 'taux_cbd', 'taux_de_CBD', 'quantite_cbd'],\n",
    "#     'temps_apparition_effets': ['temps_apparition_effets', 'temps d\\'apparition des effets', 'temps_d\\'apparition_des_effets', 'temps_avant_ressenti_des_effets', 'tempsApparitionEffets', 'delai_effet', 'temps_effets', 'delai_d\\'effet', 'temps_apparition', 'temps_d\\'apparition_des_premiers_effets', 'temps_de_reaction', 'temps_apparition_effets', 'temps_appétition', 'temps_d\\'activation', 'délais_d\\'apparition_des_effets_potentiels', 'temps d\\'apparition des effets', 'temps'],\n",
    "#     'dominance': ['dominance', 'cannabinoide_dominant', 'espèce_dominance'],\n",
    "#     'couleur': ['couleur', 'couleur_dominante', 'couleur_des_fleurs', 'couleur_de_la_fleur', 'couleurs_des_fleurs', 'variations_de_couleur', 'fleurs_couleur', 'forme_et_couleur_de_la_fleur', 'couleur_de_la_fleur'],\n",
    "#     'categorie': ['categorie', 'categorie_produit', 'catégorie', 'champ_de_produit'],\n",
    "#     'humidité': ['humidtop_10_columnsité', 'taux_humidité', 'taux_de_humidité', 'Qualité d\\'humidité', 'humidité', 'taux_humidite', 'taux_humidite', 'humidite', 'sachet_humidite'],\n",
    "#     'utilisation': ['utilisation', 'mode_utilisation', 'utilisation_et_effets', 'utilisations_et_effets', 'mode_de_consommation', 'utilisation', 'utilisation_et_effets_creatifs'],\n",
    "#     'cultivé': ['cultivé', 'cultive', 'cultivé'],\n",
    "#     'forme': ['forme', 'Forme', 'forme_et_couleur_de_la_fleur', 'forme'],\n",
    "#     'odeur': ['odeur', 'odeur_et_gout', 'odeur'],\n",
    "#     'arômes': ['arômes', 'arôme', 'parfums', 'arômes'],\n",
    "#     'espèce': ['espèce', 'Espèce'],\n",
    "#     'conservation': ['conservation', 'conditions_stockage', 'conditions_d\\'utilisation', 'durée_conservation_apres_ouverture', 'conservation']\n",
    "# }\n",
    "\n",
    "# # Fonction pour unifier les colonnes\n",
    "# def unifier_colonnes(df, champs_unifies):\n",
    "#     for champ_unifie, colonnes in champs_unifies.items():\n",
    "#         colonnes_existant = [col for col in colonnes if col in df.columns]\n",
    "#         if colonnes_existant:\n",
    "#             df[champ_unifie] = df[colonnes_existant].bfill(axis=1).iloc[:, 0]\n",
    "#             df = df.drop(columns=colonnes_existant)\n",
    "#     return df\n",
    "\n",
    "# # Unifier les colonnes du DataFrame\n",
    "# df_unifie = unifier_colonnes(df, champs_unifies)\n",
    "\n",
    "\n",
    "\n",
    "# Afficher le DataFrame résultant\n",
    "# print(df_unifie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Effets après l'inalation\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Effets après l'inalation\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEffets après l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minalation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/sqdc/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Effets après l'inalation\""
     ]
    }
   ],
   "source": [
    "df[\"Effets après l'inalation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les 10 colonnes avec le moins de valeurs NaN\n",
    "top_10_columns = df_unifie.isnull().sum().nsmallest(10).index\n",
    "\n",
    "# Filtrer le DataFrame pour ne garder que ces 10 colonnes\n",
    "df_top_10 = df_unifie[top_10_columns]\n",
    "\n",
    "# Sauvegarder le DataFrame résultant en CSV\n",
    "df_top_10.to_csv('output.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
